{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wvrt-REqt5ez"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMv2k-eFt5e5"
      },
      "source": [
        "Source: [https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html#exercise-computing-word-embeddings-continuous-bag-of-words](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html#exercise-computing-word-embeddings-continuous-bag-of-words)\n",
        "\n",
        "# Word Embeddings: Encoding Lexical Semantics\n",
        "\n",
        "Word embeddings are dense vectors of real numbers, one per word in your\n",
        "vocabulary. In NLP, it is almost always the case that your features are\n",
        "words! But how should you represent a word in a computer? You could\n",
        "store its ascii character representation, but that only tells you what\n",
        "the word *is*, it doesn't say much about what it *means* (you might be\n",
        "able to derive its part of speech from its affixes, or properties from\n",
        "its capitalization, but not much). Even more, in what sense could you\n",
        "combine these representations? We often want dense outputs from our\n",
        "neural networks, where the inputs are $|V|$ dimensional, where\n",
        "$V$ is our vocabulary, but often the outputs are only a few\n",
        "dimensional (if we are only predicting a handful of labels, for\n",
        "instance). How do we get from a massive dimensional space to a smaller\n",
        "dimensional space?\n",
        "\n",
        "How about instead of ascii representations, we use a one-hot encoding?\n",
        "That is, we represent the word $w$ by\n",
        "\n",
        "\\begin{align}\\overbrace{\\left[ 0, 0, \\dots, 1, \\dots, 0, 0 \\right]}^\\text{|V| elements}\\end{align}\n",
        "\n",
        "where the 1 is in a location unique to $w$. Any other word will\n",
        "have a 1 in some other location, and a 0 everywhere else.\n",
        "\n",
        "There is an enormous drawback to this representation, besides just how\n",
        "huge it is. It basically treats all words as independent entities with\n",
        "no relation to each other. What we really want is some notion of\n",
        "*similarity* between words. Why? Let's see an example.\n",
        "\n",
        "Suppose we are building a language model. Suppose we have seen the\n",
        "sentences\n",
        "\n",
        "* The mathematician ran to the store.\n",
        "* The physicist ran to the store.\n",
        "* The mathematician solved the open problem.\n",
        "\n",
        "in our training data. Now suppose we get a new sentence never before\n",
        "seen in our training data:\n",
        "\n",
        "* The physicist solved the open problem.\n",
        "\n",
        "Our language model might do OK on this sentence, but wouldn't it be much\n",
        "better if we could use the following two facts:\n",
        "\n",
        "* We have seen  mathematician and physicist in the same role in a sentence. Somehow they\n",
        "  have a semantic relation.\n",
        "* We have seen mathematician in the same role  in this new unseen sentence\n",
        "  as we are now seeing physicist.\n",
        "\n",
        "and then infer that physicist is actually a good fit in the new unseen\n",
        "sentence? This is what we mean by a notion of similarity: we mean\n",
        "*semantic similarity*, not simply having similar orthographic\n",
        "representations. It is a technique to combat the sparsity of linguistic\n",
        "data, by connecting the dots between what we have seen and what we\n",
        "haven't. This example of course relies on a fundamental linguistic\n",
        "assumption: that words appearing in similar contexts are related to each\n",
        "other semantically. This is called the `distributional\n",
        "hypothesis <https://en.wikipedia.org/wiki/Distributional_semantics>`__.\n",
        "\n",
        "\n",
        "# Getting Dense Word Embeddings\n",
        "\n",
        "How can we solve this problem? That is, how could we actually encode\n",
        "semantic similarity in words? Maybe we think up some semantic\n",
        "attributes. For example, we see that both mathematicians and physicists\n",
        "can run, so maybe we give these words a high score for the \"is able to\n",
        "run\" semantic attribute. Think of some other attributes, and imagine\n",
        "what you might score some common words on those attributes.\n",
        "\n",
        "If each attribute is a dimension, then we might give each word a vector,\n",
        "like this:\n",
        "\n",
        "\\begin{align}q_\\text{mathematician} = \\left[ \\overbrace{2.3}^\\text{can run},\n",
        "   \\overbrace{9.4}^\\text{likes coffee}, \\overbrace{-5.5}^\\text{majored in Physics}, \\dots \\right]\\end{align}\n",
        "\n",
        "\\begin{align}q_\\text{physicist} = \\left[ \\overbrace{2.5}^\\text{can run},\n",
        "   \\overbrace{9.1}^\\text{likes coffee}, \\overbrace{6.4}^\\text{majored in Physics}, \\dots \\right]\\end{align}\n",
        "\n",
        "Then we can get a measure of similarity between these words by doing:\n",
        "\n",
        "\\begin{align}\\text{Similarity}(\\text{physicist}, \\text{mathematician}) = q_\\text{physicist} \\cdot q_\\text{mathematician}\\end{align}\n",
        "\n",
        "Although it is more common to normalize by the lengths:\n",
        "\n",
        "\\begin{align}\\text{Similarity}(\\text{physicist}, \\text{mathematician}) = \\frac{q_\\text{physicist} \\cdot q_\\text{mathematician}}\n",
        "   {\\| q_\\text{\\physicist} \\| \\| q_\\text{mathematician} \\|} = \\cos (\\phi)\\end{align}\n",
        "\n",
        "Where $\\phi$ is the angle between the two vectors. That way,\n",
        "extremely similar words (words whose embeddings point in the same\n",
        "direction) will have similarity 1. Extremely dissimilar words should\n",
        "have similarity -1.\n",
        "\n",
        "\n",
        "You can think of the sparse one-hot vectors from the beginning of this\n",
        "section as a special case of these new vectors we have defined, where\n",
        "each word basically has similarity 0, and we gave each word some unique\n",
        "semantic attribute. These new vectors are *dense*, which is to say their\n",
        "entries are (typically) non-zero.\n",
        "\n",
        "But these new vectors are a big pain: you could think of thousands of\n",
        "different semantic attributes that might be relevant to determining\n",
        "similarity, and how on earth would you set the values of the different\n",
        "attributes? Central to the idea of deep learning is that the neural\n",
        "network learns representations of the features, rather than requiring\n",
        "the programmer to design them herself. So why not just let the word\n",
        "embeddings be parameters in our model, and then be updated during\n",
        "training? This is exactly what we will do. We will have some *latent\n",
        "semantic attributes* that the network can, in principle, learn. Note\n",
        "that the word embeddings will probably not be interpretable. That is,\n",
        "although with our hand-crafted vectors above we can see that\n",
        "mathematicians and physicists are similar in that they both like coffee,\n",
        "if we allow a neural network to learn the embeddings and see that both\n",
        "mathematicians and physicists have a large value in the second\n",
        "dimension, it is not clear what that means. They are similar in some\n",
        "latent semantic dimension, but this probably has no interpretation to\n",
        "us.\n",
        "\n",
        "\n",
        "In summary, **word embeddings are a representation of the *semantics* of\n",
        "a word, efficiently encoding semantic information that might be relevant\n",
        "to the task at hand**. You can embed other things too: part of speech\n",
        "tags, parse trees, anything! The idea of feature embeddings is central\n",
        "to the field.\n",
        "\n",
        "\n",
        "# Word Embeddings in Pytorch\n",
        "\n",
        "Before we get to a worked example and an exercise, a few quick notes\n",
        "about how to use embeddings in Pytorch and in deep learning programming\n",
        "in general. Similar to how we defined a unique index for each word when\n",
        "making one-hot vectors, we also need to define an index for each word\n",
        "when using embeddings. These will be keys into a lookup table. That is,\n",
        "embeddings are stored as a $|V| \\times D$ matrix, where $D$\n",
        "is the dimensionality of the embeddings, such that the word assigned\n",
        "index $i$ has its embedding stored in the $i$'th row of the\n",
        "matrix. In all of my code, the mapping from words to indices is a\n",
        "dictionary named word\\_to\\_ix.\n",
        "\n",
        "The module that allows you to use embeddings is torch.nn.Embedding,\n",
        "which takes two arguments: the vocabulary size, and the dimensionality\n",
        "of the embeddings.\n",
        "\n",
        "To index into this table, you must use torch.LongTensor (since the\n",
        "indices are integers, not floats).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lO-_4P3mt5e_"
      },
      "source": [
        " Hotel Reviews dataset# Exercise: Computing Word Embeddings: Continuous Bag-of-Words\n",
        "\n",
        "The Continuous Bag-of-Words model (CBOW) is frequently used in NLP deep\n",
        "learning. It is a model that tries to predict words given the context of\n",
        "a few words before and a few words after the target word. This is\n",
        "distinct from language modeling, since CBOW is not sequential and does\n",
        "not have to be probabilistic. Typcially, CBOW is used to quickly train\n",
        "word embeddings, and these embeddings are used to initialize the\n",
        "embeddings of some more complicated model. Usually, this is referred to\n",
        "as *pretraining embeddings*. It almost always helps performance a couple\n",
        "of percent.\n",
        "\n",
        "The CBOW model is as follows. Given a target word $w_i$ and an\n",
        "$N$ context window on each side, $w_{i-1}, \\dots, w_{i-N}$\n",
        "and $w_{i+1}, \\dots, w_{i+N}$, referring to all context words\n",
        "collectively as $C$, CBOW tries to minimize\n",
        "\n",
        "\\begin{align}-\\log p(w_i | C) = -\\log \\text{Softmax}(A(\\sum_{w \\in C} q_w) + b)\\end{align}\n",
        "\n",
        "where $q_w$ is the embedding for word $w$.\n",
        "\n",
        "Implement this model in Pytorch by filling in the class below. Some\n",
        "tips:\n",
        "\n",
        "* Think about which parameters you need to define.\n",
        "* Make sure you know what shape each operation expects. Use .view() if you need to\n",
        "  reshape.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MQmAqfEN9f_Q",
        "outputId": "c2c869c8-ee4f-4580-9ea9-45d09e57852c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Caribe\n",
            "  Downloading Caribe-0.8.5.tar.gz (26 kB)\n",
            "Collecting nltk==3.6.3\n",
            "  Downloading nltk-3.6.3-py3-none-any.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 42.2 MB/s \n",
            "\u001b[?25hCollecting pandas==1.3.4\n",
            "  Downloading pandas-1.3.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.3 MB 53.6 MB/s \n",
            "\u001b[?25hCollecting gingerit==0.8.2\n",
            "  Downloading gingerit-0.8.2-py3-none-any.whl (3.3 kB)\n",
            "Collecting transformers>=4.15.0\n",
            "  Downloading transformers-4.23.1-py3-none-any.whl (5.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.3 MB 59.7 MB/s \n",
            "\u001b[?25hCollecting requests>=2.25.1\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from Caribe) (1.12.1+cu113)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.6.1-py3-none-any.whl (441 kB)\n",
            "\u001b[K     |████████████████████████████████| 441 kB 69.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk==3.6.3->Caribe) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk==3.6.3->Caribe) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk==3.6.3->Caribe) (4.64.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nltk==3.6.3->Caribe) (2022.6.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.4->Caribe) (2022.5)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.4->Caribe) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.4->Caribe) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas==1.3.4->Caribe) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.1->Caribe) (2022.9.24)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.1->Caribe) (1.24.3)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.1->Caribe) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.1->Caribe) (2.10)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.15.0->Caribe) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.15.0->Caribe) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=4.15.0->Caribe) (4.13.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 70.3 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 57.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=4.15.0->Caribe) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers>=4.15.0->Caribe) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers>=4.15.0->Caribe) (3.0.9)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets->Caribe) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets->Caribe) (2022.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets->Caribe) (3.8.3)\n",
            "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets->Caribe) (0.3.5.1)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py37-none-any.whl (115 kB)\n",
            "\u001b[K     |████████████████████████████████| 115 kB 65.2 MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 73.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->Caribe) (1.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->Caribe) (22.1.0)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->Caribe) (0.13.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->Caribe) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->Caribe) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->Caribe) (1.8.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->Caribe) (6.0.2)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 70.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=4.15.0->Caribe) (3.9.0)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.13-py37-none-any.whl (115 kB)\n",
            "\u001b[K     |████████████████████████████████| 115 kB 72.6 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: Caribe\n",
            "  Building wheel for Caribe (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Caribe: filename=Caribe-0.8.5-py3-none-any.whl size=20540 sha256=c37059c8a7f90666555b69f10ffee97679cee465425cbfb27c655b4acaad8458\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4f/d5/e97c9bc6fbba5f9bce3319997a24c069c445f8ba334b885fe0\n",
            "Successfully built Caribe\n",
            "Installing collected packages: urllib3, requests, xxhash, tokenizers, responses, pandas, multiprocess, huggingface-hub, transformers, nltk, gingerit, datasets, Caribe\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.5\n",
            "    Uninstalling pandas-1.3.5:\n",
            "      Successfully uninstalled pandas-1.3.5\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.7\n",
            "    Uninstalling nltk-3.7:\n",
            "      Successfully uninstalled nltk-3.7\n",
            "Successfully installed Caribe-0.8.5 datasets-2.6.1 gingerit-0.8.2 huggingface-hub-0.10.1 multiprocess-0.70.13 nltk-3.6.3 pandas-1.3.4 requests-2.28.1 responses-0.18.0 tokenizers-0.13.1 transformers-4.23.1 urllib3-1.26.12 xxhash-3.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.13.4-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 19.6 MB/s \n",
            "\u001b[?25hCollecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.29-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 72.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.10.1-py2.py3-none-any.whl (166 kB)\n",
            "\u001b[K     |████████████████████████████████| 166 kB 69.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.28.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.0 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.9.24)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=7171079d74a141f2729bdefe42f8503d4fc66e48aa1387dc10a1bf6e70341fed\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.29 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.10.1 setproctitle-1.3.2 shortuuid-1.0.9 smmap-5.0.0 wandb-0.13.4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "import pandas as pd\n",
        "import urllib.request\n",
        "!pip install Caribe\n",
        "import Caribe as cb\n",
        "import unicodedata as uc\n",
        "\n",
        "!pip install wandb\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9jeKFJX49Cx"
      },
      "source": [
        "#Definition of various classes needed for the task at hand."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Idlq07nSig59"
      },
      "outputs": [],
      "source": [
        "class Utils:\n",
        "  \"\"\"\n",
        "  DESCRIPTION: This static class bundles a couple of helper methods\n",
        "  \"\"\"\n",
        "\n",
        "  @staticmethod\n",
        "  def make_word_dict_and_data(data:list, word_to_ix:dict, text:str) -> None:\n",
        "    \"\"\"\n",
        "    DESCRIPTION: This method is used to derive the training data and simultaneously\n",
        "    create a lookup dictionary for individual words. It operates on a row level and\n",
        "    can be used with the apply() function from pandas\n",
        "    \"\"\"\n",
        "    #make a list with single words\n",
        "    text = text.split(\" \")\n",
        "\n",
        "    #derive the next larger index\n",
        "    #e.g. dict = {\"Hello\": 0, \"World\":1} -> largest = 1\n",
        "    if word_to_ix:\n",
        "      largest = list(word_to_ix.values())\n",
        "      largest.sort()\n",
        "      largest = largest[-1] + 1\n",
        "    else:\n",
        "      largest = 0\n",
        "\n",
        "    #if word is not in dict, append it\n",
        "    for word in text:\n",
        "      if word_to_ix.get(word) is None:\n",
        "        word_to_ix.update({word: largest})\n",
        "        largest += 1\n",
        "\n",
        "\n",
        "    #append a new training tuple to the list of training data\n",
        "    # form: (pred1, pred2, pred3, pred4, crit) --> \n",
        "    # e.g. (\"this\", \"is\", \"legends\", \"are\", \"where\")\n",
        "    for i in range(2, len(text) - 2):\n",
        "       data.append((\n",
        "          text[i - 2],\n",
        "          text[i - 1],\n",
        "          text[i + 1],\n",
        "          text[i + 2],\n",
        "          text[i]))\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def spell_checking(text: str) -> str:\n",
        "    \"\"\"\n",
        "    DESCRIPTION: This method is used for spell checking. Caribe is a very high quality spell\n",
        "    checker which operates on sentence level, rather than word level. However, note that this\n",
        "    approach is computationally very expensive.\n",
        "    \n",
        "    RESSOURCES: https://www.thecaribe.org/\n",
        "    https://stackoverflow.com/questions/10252448/how-to-check-whether-a-sentence-is-correct-simple-grammar-check-in-python\n",
        "    \"\"\"\n",
        "    return cb.caribe_corrector(text)\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def unicode_normalization(text: str) -> str:\n",
        "    \"\"\"\n",
        "    DESCRIPTION: Normalizes all unicode characters\n",
        "    RESSOURCES: https://www.unicode.org/reports/tr15/, https://www.evernote.com/shard/s223/client/snv?noteGuid=f1c62270-a999-415f-ad24-39996d829ad5&noteKey=d379441c7671371c744cd5266c932406&sn=https%3A%2F%2Fwww.evernote.com%2Fshard%2Fs223%2Fsh%2Ff1c62270-a999-415f-ad24-39996d829ad5%2Fd379441c7671371c744cd5266c932406&title=2.%2BNLP%2BPipeline%2B%257C%2BPractical%2BNatural%2BLanguage%2BProcessing\n",
        "    \"\"\"\n",
        "    return uc.normalize(\"NFKD\", text) #canonical\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3FAVcTl5QaN"
      },
      "outputs": [],
      "source": [
        "class CBOW(nn.Module):\n",
        "    \"\"\"\n",
        "    DESCRIPTION: This class represents the computational model of the Continuous Bag of Words\n",
        "    As stated above it minimizes the logSoftmax.\n",
        "    RESSOURCES: https://discuss.pytorch.org/t/how-to-implement-skip-gram-or-cbow-in-pytorch/47625, https://github.com/FraLotito/pytorch-continuous-bag-of-words/blob/master/cbow.py\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size: int, embedding_dim: int) -> None:\n",
        "      super(CBOW, self).__init__()\n",
        "\n",
        "      #create embeddings and enable CUDA\n",
        "      #out: 1 x emdedding_dim\n",
        "      self.embeddings = nn.Embedding(vocab_size, embedding_dim,\n",
        "                                     device=torch.device('cuda:0')\n",
        "                                     )\n",
        "      \n",
        "      #Define linear Transformation and enable CUDA\n",
        "      self.linear1 = nn.Linear(embedding_dim, 128,\n",
        "                               device=torch.device('cuda:0')\n",
        "                               )\n",
        "      self.activation_function1 = nn.ReLU()\n",
        "      \n",
        "      \n",
        "      #Define linear Transformation and enable CUDA\n",
        "      #out: 1 x vocab_size\n",
        "      self.linear2 = nn.Linear(128, vocab_size,\n",
        "                               device=torch.device('cuda:0')\n",
        "                               )\n",
        "      #define logSoftmax\n",
        "      self.activation_function2 = nn.LogSoftmax(dim = -1)\n",
        "        \n",
        "\n",
        "    def forward(self, inputs):\n",
        "      \"\"\"\n",
        "      DESCRIPTION: \"In NN module, forward() function does the actual message passing and computation.\"\n",
        "      RESSOURCES: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html, https://discuss.pytorch.org/t/how-to-handle-batch-size-when-using-linear-layer/74492\n",
        "      \"\"\"\n",
        "      embeds = sum(self.embeddings(inputs))\n",
        "      out = embeds.view(embeds.size(0), -1)\n",
        "      out = self.linear1(out)\n",
        "      out = self.activation_function1(out)\n",
        "      out = self.linear2(out)\n",
        "      out = self.activation_function2(out)\n",
        "      return out\n",
        "\n",
        "    def get_word_emdedding(self, word: str, word_to_ix: dict):\n",
        "      \"\"\"\n",
        "      DESCRIPTION: Method to retreive single word embeddings for model validation\n",
        "      \"\"\"\n",
        "\n",
        "      word = torch.tensor([word_to_ix[word]], device = torch.device('cuda:0'))\n",
        "      return self.embeddings(word).view(1,-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3FzBJcUaAmG"
      },
      "outputs": [],
      "source": [
        "class ModelTrainer:\n",
        "\n",
        "  \"\"\"\n",
        "  DESCRIPTION: This class is used for the actual model training.\n",
        "  Important are the attributes .losses and .model which hold relevant output of the model\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, vocab_size: int, embedding_dim: int, epochs: int, dataloader) -> None:\n",
        "\n",
        "    #INIT PROCEDURE\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.epochs = epochs\n",
        "    self.dataloader = dataloader\n",
        "    self.model = CBOW(self.vocab_size, self.embedding_dim)\n",
        "    self.model.to(torch.device('cuda:0'))\n",
        "    self.losses = list()\n",
        "    self.loss_function = nn.NLLLoss()\n",
        "    self.optimizer = torch.optim.SGD(self.model.parameters(), lr=0.001)\n",
        "    self.__train()\n",
        "\n",
        "  def __train(self) -> None:\n",
        "    \"\"\"\n",
        "    DESCRIPTION: This method defines the actual training loop\n",
        "    It is private, i.e. it should not be accessed from outside the object.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Training has started...\")\n",
        "    \n",
        "    for epoch in range(self.epochs):\n",
        "\n",
        "      total_loss = 0\n",
        "\n",
        "      for context, target in self.dataloader:\n",
        "        \n",
        "        self.model.zero_grad()\n",
        "\n",
        "        log_probs = self.model(context)\n",
        "    \n",
        "        loss = self.loss_function(log_probs, target)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        self.optimizer.step()\n",
        "    \n",
        "        total_loss += loss.item() \n",
        "  \n",
        "      self.losses.append(total_loss)\n",
        "      print(f\"Completed epoch: {epoch}, with loss: {total_loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "liUF_gIJiaeU"
      },
      "outputs": [],
      "source": [
        "class tripAdvisor():\n",
        "    \"\"\"\n",
        "    DESCRIPTION: This class downloads and preprocesses the Trip-Advisor data. The data can be \n",
        "    retreived over attributes self.data_X and self.data_y\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "      \n",
        "      #download the data from google drive\n",
        "      # https://stackoverflow.com/questions/56611698/pandas-how-to-read-csv-file-from-google-drive-public\n",
        "      url = 'https://drive.google.com/file/d/1ihP1HZ8YHVGGIEp1RHxXdt3PPIi12xvL/view'\n",
        "      path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]\n",
        "      data = pd.read_csv(path)\n",
        "\n",
        "      #preprocess the data\n",
        "      data = self.__preprocess(data)\n",
        "\n",
        "      #split the data into X and y. Note that for our purpose, we only need self.data_X\n",
        "      self.data_X = data[\"Review\"]\n",
        "      self.data_y = data[\"Rating\"]\n",
        "\n",
        "      #check wether the two are the same.\n",
        "      assert len(self.data_X) == len(self.data_y)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.data_X)\n",
        "    \n",
        "    def __iter__(self):\n",
        "        for i in range(len(self.data_X)):\n",
        "            yield (self.data_X[i], self.data_y[i])\n",
        "\n",
        "    def __preprocess(self, temp):\n",
        "      \"\"\"\n",
        "      DESCRIPTION: This method defines the training procedure of the TripAdvisor data\n",
        "      \"\"\"\n",
        "      \n",
        "      #lowercase and strip the whitespaces\n",
        "      temp[\"Review\"] = temp[\"Review\"].str.lower()\n",
        "      temp[\"Review\"] = temp[\"Review\"].str.strip()\n",
        "\n",
        "      #delete all digits\n",
        "      regex = r\"[0-9]*\"\n",
        "      temp[\"Review\"] = temp[\"Review\"].str.replace(regex, \"\", regex = True)\n",
        "      \n",
        "      #delete all signs and meaningless char\n",
        "      regex = r\"[\\W_]+\"\n",
        "      temp[\"Review\"] = temp[\"Review\"].str.replace(regex, \" \", regex = True)\n",
        "\n",
        "      #split single review chunks\n",
        "      temp[\"Review\"] = temp[\"Review\"].str.split(\",\")\n",
        "      temp[\"Review\"] = temp[\"Review\"].explode()\n",
        "      temp[\"Review\"] = temp[\"Review\"].str.split(\".\")\n",
        "      temp[\"Review\"] = temp[\"Review\"].explode()\n",
        "      temp[\"Review\"] = temp[\"Review\"].reset_index(drop = True)\n",
        "\n",
        "      temp[\"Review\"] = [Utils.unicode_normalization(review) for review in temp[\"Review\"].to_list()]\n",
        "      #temp[\"Review\"] = [Utils.spell_checking(review) for review in temp[\"Review\"].to_list()]\n",
        "\n",
        "      return temp\n",
        "\n",
        "trAd = tripAdvisor()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03Uc1hEy34TP",
        "outputId": "e9ee6fad-0a5b-499c-d257-11e270948773"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1247     best bathrooms stayed double room nights septe...\n",
              "2181     good location value hotel changed hands curren...\n",
              "15007    beware spare sanity money puerto rico better g...\n",
              "16945    dirty bathroom jurys boston great location bed...\n",
              "20216    disapointment excited stay hotel visiting webs...\n",
              "3220     wonderful experience husband stayed nights won...\n",
              "14037    nice hotel perfect location recommend great ho...\n",
              "16701    great location just returned week florence hot...\n",
              "18815    loved buy vacation club just returned staying ...\n",
              "15836    prime primo stayed prime hotel year agree posi...\n",
              "Name: Review, dtype: object"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trAd.data_X.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BG2R-hLHTyGY"
      },
      "outputs": [],
      "source": [
        "class pytorchTripAdvisor(Dataset):\n",
        "  \"\"\"\n",
        "  DESCRIPTION: This is the dataset class of the TripAdvisor Dataset.\n",
        "  It uses Composition to import the tripAdvisor object. This class inherits \n",
        "  from the Dataset module from PyTorch, which makes it possible to batch the dataset\n",
        "  later on. \n",
        "  NOTE: This class could also be merged with the tripAdvisor class itself. However this is\n",
        "  bad practice in object oriented code --> Single Responsibility Principle\n",
        "  RESSOURCES: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "      self.tripAdvisor = tripAdvisor()\n",
        "      self.data = list()\n",
        "      self.word_to_ix = dict()\n",
        "\n",
        "      #fill the dictionary and the list with all the training samples\n",
        "      self.tripAdvisor.data_X.apply(\n",
        "          lambda x: Utils.make_word_dict_and_data(\n",
        "              self.data, self.word_to_ix, x\n",
        "              )\n",
        "          )\n",
        "      \n",
        "      #convert the list with all the training data into a pd.DataFrame\n",
        "      #e.g. [(\"this\", \"is\", \"great\", \"example\", \"an\"), (\"is\", \"a\",\"example\",\"friend\",\"great\")] -->\n",
        "      # pred1 |  pred2  |  pred3  |  pred4  |  crit\n",
        "      # -------------------------------------------\n",
        "      # this  |  is     |  great  |  example|  an\n",
        "      # is    |  a      |  example|  friend |  great\n",
        "      self.full_data = pd.DataFrame().from_records(\n",
        "          self.data, \n",
        "          columns=['pred1',\n",
        "                   'pred2',\n",
        "                   \"pred3\",\n",
        "                   \"pred4\",\n",
        "                   \"crit\"])\n",
        "      \n",
        "      #convert the data in word format into numeric format by looking it up in the \n",
        "      #dict word_to_ix\n",
        "      for col in self.full_data:\n",
        "        self.full_data[col] = self.full_data[col].map(self.word_to_ix)\n",
        "\n",
        "      #convert the predictors (i.e. pred1- pred4) into a 2d tensor and enable CUDA\n",
        "      self.data_X = self.full_data.iloc[:,:4]\n",
        "      self.data_X = torch.tensor(self.data_X.values.astype(np.int64),\n",
        "                                 device=torch.device('cuda:0')\n",
        "                                 )\n",
        "      \n",
        "      #convert the criterion (crit) into a 1d tensor and enable cuda\n",
        "      self.data_y = self.full_data.iloc[:,4]\n",
        "      self.data_y = torch.tensor(self.data_y.values.astype(np.int64),\n",
        "                                 device=torch.device('cuda:0')\n",
        "                                 )\n",
        "\n",
        "      assert(len(self.data_X) == len(self.data_y))\n",
        "\n",
        "  def __len__(self):\n",
        "        return len(self.full_data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "        return self.data_X[idx], self.data_y[idx]\n",
        "\n",
        "TrAd = pytorchTripAdvisor()\n",
        "  \n",
        "# implement dataloader on the dataset with batchsize 4\n",
        "dataloader_tripAdvisor = DataLoader(TrAd, batch_size=4, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyyE4YdqT1Dx",
        "outputId": "bdcf8945-af70-4130-9713-f9cd156cecf5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 472,  201,  864, 9403],\n",
            "        [ 233,  883, 6221,   36],\n",
            "        [4566,  722, 1425, 1601],\n",
            "        [2422, 2521,  980,  149]], device='cuda:0')\n",
            "tensor([32417,  1210,    16,  2175], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "single_batch = iter(dataloader_tripAdvisor).next()\n",
        "context, target = single_batch\n",
        "print(context)\n",
        "print(target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVtGqsN7oZbw",
        "outputId": "fa9421dc-eef0-4191-e318-121b855d04d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:55: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n"
          ]
        }
      ],
      "source": [
        "class SciFi():\n",
        "    \"\"\"\n",
        "    DESCRIPTION: This class downloads and preprocesses the Trip-Advisor data. The data can be \n",
        "    retreived over attributes self.data_X and self.data_y\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "      \n",
        "      #download the data from google drive\n",
        "      # https://stackoverflow.com/questions/56611698/pandas-how-to-read-csv-file-from-google-drive-public\n",
        "      url = 'https://drive.google.com/file/d/10ehW4jZND3QA29v9aNboYUett5-swuNe/view'\n",
        "      path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]\n",
        "      \n",
        "      # Assign the open file to a variable\n",
        "      webFile = urllib.request.urlopen(path)\n",
        "\n",
        "      # Read the file contents to a variable.\n",
        "      #this file is of type bit\n",
        "      file_contents = webFile.read()\n",
        "\n",
        "      #Convert to a pd.DataFrame with each sentence being one row in column \"Text\"\n",
        "      data = pd.DataFrame({\"Text\": str(file_contents).split(\".\")})\n",
        "\n",
        "      #reduce data to 100'000 rows to reduce training time\n",
        "      data = data.sample(100000, replace = False)\n",
        "      data = data.reset_index(drop = True)\n",
        "\n",
        "      #preprocess the data\n",
        "      data = self.__preprocess(data)\n",
        "\n",
        "      #split the data into X and y. Note that for our purpose, we only need self.data_X\n",
        "      self.data_X = data[\"Text\"]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.data_X)\n",
        "    \n",
        "    def __iter__(self):\n",
        "        for i in range(len(self.data_X)):\n",
        "            yield (self.data_X[i], self.data_y[i])\n",
        "\n",
        "    def __preprocess(self, temp):\n",
        "      \"\"\"\n",
        "      DESCRIPTION: This method defines the training procedure of the SciFi data\n",
        "      \"\"\"\n",
        "      \n",
        "      #drop the first and the last 15 as these are not propper, meaningfull sentences but\n",
        "      #rather some information on the book and acknowledgements\n",
        "      temp = temp.drop(temp.index[:15])\n",
        "      temp = temp.drop(temp.tail(15).index)\n",
        "\n",
        "      #delete all rows, which, after the other preprocessing steps are almost empty.\n",
        "      temp = temp[temp[\"Text\"].str.len() > 2]\n",
        "\n",
        "      #to ensure signs are correctly understood by the machine, the author put a \\ in front\n",
        "      #e.g. don\\'t --> don't\n",
        "      temp[\"Text\"] = temp[\"Text\"].str.replace(\"\\\\\", \"\")\n",
        "\n",
        "      #There are some random signs in the data, they are cut out\n",
        "      temp[\"Text\"] = temp[\"Text\"].str.replace(\"#\", \"\")\n",
        "\n",
        "      #lowercase and strip the whitespaces\n",
        "      temp[\"Text\"] = temp[\"Text\"].str.lower()\n",
        "      temp[\"Text\"] = temp[\"Text\"].str.strip()\n",
        "\n",
        "      #delete all digits\n",
        "      regex = r\"[0-9]*\"\n",
        "      temp[\"Text\"] = temp[\"Text\"].str.replace(regex, \"\", regex = True)\n",
        "            \n",
        "      #delete all signs and meaningless char\n",
        "      regex = r\"[\\W_]+\"\n",
        "      temp[\"Text\"] = temp[\"Text\"].str.replace(regex, \" \", regex = True)\n",
        "\n",
        "      #delete all rows, which, after the other preprocessing steps are almost empty.\n",
        "      temp = temp[temp[\"Text\"].str.len() > 2]\n",
        "      \n",
        "      #unicode normalisation\n",
        "      temp[\"Text\"] = [Utils.unicode_normalization(text) for text in temp[\"Text\"].to_list()]\n",
        "      #temp[\"Text\"] = [Utils.spell_checking(text) for text in temp[\"Text\"].to_list()]\n",
        "\n",
        "      return temp\n",
        "\n",
        "scFi = SciFi()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bm1-AVmSihkK",
        "outputId": "d8879cb7-52bb-423d-f0ef-e4ee34a5afc3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "34742                       it was foolish it was pathetic\n",
              "37088     i last saw zack under the most unpleasant of ...\n",
              "57002    some of them were back to playing catch and so...\n",
              "5063                 oh i di tired hate i want to rest now\n",
              "18846    normal temperature in here is about fifty degrees\n",
              "60703                                      we ll start now\n",
              "22684    so i petitioned to be adopted and here lam sch...\n",
              "8586                                apathy desperation and\n",
              "61230                         ix gun inside his cummerbund\n",
              "21165                it went on like that for several days\n",
              "Name: Text, dtype: object"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scFi.data_X.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8EvEKkDDrBi",
        "outputId": "5f8b2972-996b-40cc-fa38-db02021918df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:55: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n"
          ]
        }
      ],
      "source": [
        "class pytorchSciFi(Dataset):\n",
        "  \"\"\"\n",
        "  DESCRIPTION: This is the dataset class of the SciFi Dataset.\n",
        "  It uses Composition to import the SciFi object. This class inherits \n",
        "  from the Dataset module from PyTorch, which makes it possible to batch the dataset\n",
        "  later on. \n",
        "  NOTE: This class could also be merged with the SciFi class itself. However this is\n",
        "  bad practice in object oriented code --> Single Responsibility Principle\n",
        "  RESSOURCES: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "      self.SciFi = SciFi()\n",
        "      self.data = list()\n",
        "      self.word_to_ix = dict()\n",
        "\n",
        "      #fill the dictionary and the list with all the training samples\n",
        "      self.SciFi.data_X.apply(\n",
        "          lambda x: Utils.make_word_dict_and_data(\n",
        "              self.data, self.word_to_ix, x\n",
        "              )\n",
        "          )\n",
        "      \n",
        "      #convert the list with all the training data into a pd.DataFrame\n",
        "      #e.g. [(\"this\", \"is\", \"great\", \"example\", \"an\"), (\"is\", \"a\",\"example\",\"friend\",\"great\")] -->\n",
        "      # pred1 |  pred2  |  pred3  |  pred4  |  crit\n",
        "      # -------------------------------------------\n",
        "      # this  |  is     |  great  |  example|  an\n",
        "      # is    |  a      |  example|  friend |  great\n",
        "      self.full_data = pd.DataFrame().from_records(\n",
        "          self.data, \n",
        "          columns=['pred1',\n",
        "                   'pred2',\n",
        "                   \"pred3\",\n",
        "                   \"pred4\",\n",
        "                   \"crit\"])\n",
        "      \n",
        "      #convert the data in word format into numeric format by looking it up in the \n",
        "      #dict word_to_ix\n",
        "      for col in self.full_data:\n",
        "        self.full_data[col] = self.full_data[col].map(self.word_to_ix)\n",
        "\n",
        "      #convert the predictors (i.e. pred1- pred4) into a 2d tensor and enable CUDA\n",
        "      self.data_X = self.full_data.iloc[:,:4]\n",
        "      self.data_X = torch.tensor(self.data_X.values.astype(np.int64),\n",
        "                                 device=torch.device('cuda:0')\n",
        "                                 )\n",
        "      #convert the criterion (crit) into a 1d tensor and enable cuda\n",
        "      self.data_y = self.full_data.iloc[:,4]\n",
        "      self.data_y = torch.tensor(self.data_y.values.astype(np.int64),\n",
        "                                 device=torch.device('cuda:0')\n",
        "                                 )\n",
        "\n",
        "      assert(len(self.data_X) == len(self.data_y))\n",
        "\n",
        "  def __len__(self):\n",
        "        return len(self.full_data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "        return self.data_X[idx], self.data_y[idx]\n",
        "\n",
        "ScFi = pytorchSciFi()\n",
        "  \n",
        "# implementing dataloader on the dataset\n",
        "dataloader_SciFi = DataLoader(ScFi, batch_size=4, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INnK-0zuNBze",
        "outputId": "b87cd72c-7482-4f4e-b99d-6b5abfe6a2b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[  205,    27,    14, 15491],\n",
            "        [    1,  5990,  4208,    94],\n",
            "        [   47,  5627,   153, 16892],\n",
            "        [  209,  8693,   358,  2017]], device='cuda:0')\n",
            "tensor([ 2956,   935,   389, 13295], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "#visualize a single batch\n",
        "single_batch = iter(dataloader_SciFi).next()\n",
        "context, target = single_batch\n",
        "print(context)\n",
        "print(target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOrR2zlJ4jvd"
      },
      "source": [
        "# In the following, the code regarding the CBOW model for the tripAdvisor data can be found\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "pXqd5UWMoHvo",
        "outputId": "e5f3046d-1eee-46fa-9524-2fa09d054e63"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.13.4"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20221028_171040-5swp6c23</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/uzhkili/uncategorized/runs/5swp6c23\" target=\"_blank\">azure-sunset-4</a></strong> to <a href=\"https://wandb.ai/uzhkili/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/uzhkili/uncategorized/runs/5swp6c23?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7fb7a2b259d0>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#supervise google collab gpu on external website\n",
        "wandb.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7x9ch35XiKl",
        "outputId": "0c4299ef-30b6-48b2-f73c-939cc9740161"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training has started...\n",
            "Completed epoch: 0, with loss: 4319465.357142448\n",
            "Completed epoch: 1, with loss: 4086388.2344048023\n",
            "Completed epoch: 2, with loss: 4057752.655842066\n",
            "Completed epoch: 3, with loss: 4049513.916833639\n",
            "Completed epoch: 4, with loss: 4045923.2771425247\n",
            "Completed epoch: 5, with loss: 4043892.138977766\n",
            "Completed epoch: 6, with loss: 4042242.748764038\n",
            "Completed epoch: 7, with loss: 4041040.6307649612\n",
            "Completed epoch: 8, with loss: 4040194.784653187\n",
            "Completed epoch: 9, with loss: 4039278.382889271\n",
            "Completed epoch: 10, with loss: 4038504.735740185\n",
            "Completed epoch: 11, with loss: 4037773.201829672\n"
          ]
        }
      ],
      "source": [
        "#Train the model on GPU (Time ca. 3.5 h)\n",
        "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
        "EMBEDDING_DIM = 50 #this was proposed in the excercise sheet.\n",
        "VOCAB_SIZE = len(TrAd.word_to_ix)\n",
        "EPOCHS = 12\n",
        "DATALOADER = dataloader_tripAdvisor\n",
        "model_tripAdvisor = ModelTrainer(VOCAB_SIZE, EMBEDDING_DIM, EPOCHS, DATALOADER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "lvgqbaR000vK",
        "outputId": "b609d4a5-c780-4e6a-a9f4-7b5b6076f6f4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |  186517 KB |  284095 KB |  227862 GB |  227861 GB |\\n|       from large pool |  185087 KB |  278179 KB |  207727 GB |  207727 GB |\\n|       from small pool |    1430 KB |    6491 KB |   20134 GB |   20134 GB |\\n|---------------------------------------------------------------------------|\\n| Active memory         |  186517 KB |  284095 KB |  227862 GB |  227861 GB |\\n|       from large pool |  185087 KB |  278179 KB |  207727 GB |  207727 GB |\\n|       from small pool |    1430 KB |    6491 KB |   20134 GB |   20134 GB |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |  321536 KB |  321536 KB |  321536 KB |       0 B  |\\n|       from large pool |  313344 KB |  313344 KB |  313344 KB |       0 B  |\\n|       from small pool |    8192 KB |    8192 KB |    8192 KB |       0 B  |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |   32619 KB |   43482 KB |   90915 GB |   90915 GB |\\n|       from large pool |   27905 KB |   38768 KB |   70763 GB |   70763 GB |\\n|       from small pool |    4714 KB |    4784 KB |   20152 GB |   20152 GB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |      24    |      53    |  159053 K  |  159053 K  |\\n|       from large pool |       8    |      13    |   12724 K  |   12724 K  |\\n|       from small pool |      16    |      40    |  146329 K  |  146329 K  |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |      24    |      53    |  159053 K  |  159053 K  |\\n|       from large pool |       8    |      13    |   12724 K  |   12724 K  |\\n|       from small pool |      16    |      40    |  146329 K  |  146329 K  |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |      15    |      15    |      15    |       0    |\\n|       from large pool |      11    |      11    |      11    |       0    |\\n|       from small pool |       4    |       4    |       4    |       0    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |      20    |      23    |   98935 K  |   98935 K  |\\n|       from large pool |       7    |      11    |    6397 K  |    6397 K  |\\n|       from small pool |      13    |      13    |   92537 K  |   92537 K  |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#what happened during training in Memory?\n",
        "torch.cuda.memory_summary(device=torch.device('cuda:0'), abbreviated=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uaqTMkswu9n2"
      },
      "outputs": [],
      "source": [
        "#save the model to google drive (Note: save is only temporary (until kernel shuts down) i.e. DOWNLOAD!)\n",
        "torch.save(model_tripAdvisor.model, \"tripadvisor_model.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJFCS6J8bxPd",
        "outputId": "b47b47ea-5f12-4631-f930-7e08a97f88af"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[4319465.357142448,\n",
              " 4086388.2344048023,\n",
              " 4057752.655842066,\n",
              " 4049513.916833639,\n",
              " 4045923.2771425247,\n",
              " 4043892.138977766,\n",
              " 4042242.748764038,\n",
              " 4041040.6307649612,\n",
              " 4040194.784653187,\n",
              " 4039278.382889271,\n",
              " 4038504.735740185,\n",
              " 4037773.201829672]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# total loss per epoch (devide by number of observations to get loss per observation)\n",
        "model_tripAdvisor.losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "d1NlwIeQTDuI",
        "outputId": "6c99461a-b034-4be5-fff2-0cb26cc2811b"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot the loss with some scaling\n",
        "plt.bar([i for i in range(1,13)], \n",
        "        [(loss / len(TrAd.data)) for loss in model_tripAdvisor.losses],\n",
        "        color='royalblue', \n",
        "        alpha=0.7)\n",
        "\n",
        "plt.grid(color='#95a5a6',\n",
        "         linestyle='--',\n",
        "         linewidth=2,\n",
        "         axis='y',\n",
        "         alpha=0.7)\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim(1.9, 2.055)\n",
        "plt.title('Scaled Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uivBKFHW5b2H"
      },
      "source": [
        "# In the following, the code regarding the CBOW model from the SciFi data can be found."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "CLNGOa3tacCA",
        "outputId": "5e167a52-4241-4db6-cc9d-d08b79542dbc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.13.4"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20221027_195447-ydchvze0</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/uzhkili/uncategorized/runs/ydchvze0\" target=\"_blank\">solar-elevator-3</a></strong> to <a href=\"https://wandb.ai/uzhkili/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/uzhkili/uncategorized/runs/ydchvze0?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7fc885046510>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#supervise google collab gpu on external website\n",
        "wandb.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHeyZ4_4ab0q",
        "outputId": "154e342a-fbc7-4836-fdb4-2ad96f417a09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training has started...\n",
            "Completed epoch: 0, with loss: 1798045.486424446\n",
            "Completed epoch: 1, with loss: 1685143.827660799\n",
            "Completed epoch: 2, with loss: 1663289.2744021416\n",
            "Completed epoch: 3, with loss: 1652981.5474276543\n",
            "Completed epoch: 4, with loss: 1647370.9525547028\n",
            "Completed epoch: 5, with loss: 1644098.571789503\n",
            "Completed epoch: 6, with loss: 1642055.2729730606\n",
            "Completed epoch: 7, with loss: 1640451.0267271996\n",
            "Completed epoch: 8, with loss: 1639507.6277196407\n",
            "Completed epoch: 9, with loss: 1638707.479288578\n",
            "Completed epoch: 10, with loss: 1638217.0442771912\n",
            "Completed epoch: 11, with loss: 1637565.5002009869\n"
          ]
        }
      ],
      "source": [
        "#Train the model on GPU\n",
        "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
        "EMBEDDING_DIM = 50 #this was proposed in the excercise sheet.\n",
        "VOCAB_SIZE = len(ScFi.word_to_ix)\n",
        "EPOCHS = 12\n",
        "DATALOADER = dataloader_SciFi\n",
        "model_SciFi = ModelTrainer(VOCAB_SIZE, EMBEDDING_DIM, EPOCHS, DATALOADER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJufwYiebLfG"
      },
      "outputs": [],
      "source": [
        "#save the model to google drive (Note: save is only temporary (until kernel shuts down) i.e. DOWNLOAD!)\n",
        "torch.save(model_SciFi.model, \"scifi_model.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_CrjgUVjGVa",
        "outputId": "c995f842-77ee-40d8-dd31-7732975f8dda"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1798045.486424446,\n",
              " 1685143.827660799,\n",
              " 1663289.2744021416,\n",
              " 1652981.5474276543,\n",
              " 1647370.9525547028,\n",
              " 1644098.571789503,\n",
              " 1642055.2729730606,\n",
              " 1640451.0267271996,\n",
              " 1639507.6277196407,\n",
              " 1638707.479288578,\n",
              " 1638217.0442771912,\n",
              " 1637565.5002009869]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# total loss per epoch (devide by number of observations to get loss per observation)\n",
        "model_SciFi.losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "raey-vyZjFFQ",
        "outputId": "ab1a127a-4182-43ec-aba0-4adfa6d35d98"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de3xdZZ3v8c+3NwhtTGkLPQ1tqSNtESktJKIjnaHCDFMFFRWFjgLWCzOCMzg6HpTzOsIMykEdL3gQGS+1KFicg3BAKkoP0tYqqAkW0hZIoUBpUyi0kqYXeqG/88daiTvpymUle2c3zff9eu1XV55nrWf/fiHkl2c9a6+liMDMzKynhpQ7ADMzG1hcOMzMLBcXDjMzy8WFw8zMcnHhMDOzXFw4zMwsFxcOsyKR9CFJK/r7WLP+5sJhg5Kk2ZJ+K6lZ0lZJv5H0xnLHlUXSFEkhaVi5YzED8A+iDTqSXgPcA3wc+C9gBPBXwO5yxmU2UHjGYYPRNICIWBQRr0bEroi4LyIebd1B0sckPSapRdIaSaek7Z+V9FRB+7s7exNJx0taks5onpD0/oK+sZLulrRN0u+B1/UmEUnV6ThbJT0p6WMFfadKqkvf4wVJX0vbD5d0i6Qtkl6W9AdJ43vz/jY4ecZhg1Ej8Kqkm4HbgIci4k+tnZLeB1wNnAvUkfxS35t2P0UyO3keeB9wi6TjImJT4RtIGgksAT4PvA2YASyRtCoi1gDfAl4BJgCvBX4JPN2LXG4DVgHVwPHpezwVEb8Crgeuj4gfSRoFnJgeczFQBUwimWXNAnb14r1tkPKMwwadiNgGzAYC+C7wYvpXe+tf3R8FvhwRf4jEkxHxbHrs/4mIpojYHxE/AdYCp2a8zTnAMxHxg4jYFxF/BH4KvE/SUOC9wOcjYkdErAJuzpuHpEnAacAVEfFKRKwEvgdclO6yFzhO0riI2B4RDxW0jwWOS2dc9en3xKxHXDhsUIqIxyLiQxExkeQv8WrgG2n3JJKZxQEkXSRpZXqK5+X02HEZux4LvKl1v3TfDwD/DTiKZLb/XMH+z/YijWpga0S0dBjnmHT7IySn5R5PT0edk7b/iGSGc5ukJklfljS8F+9vg5QLhw16EfE4sJA/n8p5jow1B0nHksxQPgGMjYjRJKeJlDHsc8CyiBhd8BoVER8HXgT2kRSoVpN7EXoTMEZSZYdxNqZ5rY2IecDRwJeA2yWNjIi9EfFvEXEC8BaS2dFFmPWQC4cNOumi9aclTUy/ngTMA1pP5XwP+FdJNUoclxaNkSSnt15Mj5vPn4tNR/cA0yRdKGl4+nqjpNdHxKvAHcDVko6QdALJukN3DksXtg+XdDhJgfgt8L/StpNIZhm3pPF9UNJREbEfeDkdY7+kt0qakZ4y20Zy6mp/T79/Zi4cNhi1AG8CfidpB0nBWAV8GpJ1DOCLwI/Tff8vMCZd1P4q8CDwAsmC92+y3iA9fXQWcAHJzOB5kr/6D0t3+QQwKm1fCPygB3FvJ1nEbn2dQVLwpqTvcSdwVUT8v3T/ucBqSdtJFsoviIhdJKfLbicpGo8By0hOX5n1iPwgJzMzy8MzDjMzy8WFw8zMcnHhMDOzXFw4zMwsl0Fxy5Fx48bFlClTyh2GmdmAUl9f/1JEHNWxfVAUjilTplBXV1fuMMzMBhRJmXc08KkqMzPLpWSFQ9ICSZslreqk/0hJd0p6VNLvJZ2Ytk+S9EB6y+rVki4vOOZqSRvTewWtlPT2UsVvZmbZSjnjWEjyydXOXAmsjIiTSO6Tc33avg/4dHofnTcDl6W3ZGj19YiYlb5+XoK4zcysCyVb44iI5ZKmdLHLCcB16b6Pp4/HHJ8+12BT2t4i6TGSu32u6W0szS0tLF66LLNvxrSpTK6uBmB9UxMNjWs7HefsOae3ba+or6e5ZXvmfpMnTGDG9Glt772i/uFOx5xdcwpVlck96hqeaGT9pk2Z+1VVjmJ2TU3b153lA87JOTkncE7FyilLORfHHwHeA/xa0qkkt6GeSHIPICB51jJwMvC7guM+IekikgfsfLrwATyFJF0CXAJw1PjeP9zs8Wf3tG3/5sbNbdtVFbsYNmRP1iGsfLKFRfcn+w4dspPRFe33O/7YEb2Ox8ys3Ep6r6r0F/89EXHAHUTT5z5fT1IYGkieXvax9GE0pE8sWwZ8MSLuSNvGAy+R3KH0GmBCRHy4uzhqa2ujt1dVXVlQLIrl2kuPLvqYZmbFJqk+Imo7tpdtxpE+cWw+gCSRPDZzXfr1cJKnpd3aWjTSYwpnI98luXW1mZn1o7JdjitptKTWczYfBZZHxLa0iHwfeCwivtbhmAkFX76b5FbYZmbWj0o245C0CJgDjJO0AbgKGA4QETcBrwdulhTAapIH0EDyDOULgQZJK9O2K9MrqL4saRbJqapngH8oVfxmZpatlFdVzeum/0GS5yF3bF9B9qM4iYgLixOdmZn1lj85bmZmubhwmJlZLi4cZmaWiwuHmZnl4sJhZma5uHCYmVkuLhxmZpaLC4eZmeXiwmFmZrm4cJiZWS4uHGZmlosLh5mZ5eLCYWZmubhwmJlZLi4cZmaWiwuHmZnl4sJhZma5uHCYmVkuLhxmZpaLC4eZmeVS0sIhaYGkzZJWddJ/pKQ7JT0q6feSTizomyvpCUlPSvpsQftrJf0ubf+JpBGlzMHMzNor9YxjITC3i/4rgZURcRJwEXA9gKShwLeAtwEnAPMknZAe8yXg6xFxHPAn4COlCd3MzLIMK+XgEbFc0pQudjkBuC7d93FJUySNB/4CeDIi1gFIug14l6THgDOAv0+Pvxm4Gvh2V3E0t7SweOmyzL4Z06YyuboagPVNTTQ0rm3XP3bknrbtLTtmtm1XVTQybMiuzDFf2TuGHXsmATB0yE5GV7Qfc/HSP0+SZtecQlVlJQANTzSyftOmzDGrKkcxu6amYIzsfHqSU6Gz55zetr2ivp7mlu2Z+02eMIEZ06cByfdzRf3DnY7pnJwTOKdDJacs5V7jeAR4D4CkU4FjgYnAMcBzBfttSNvGAi9HxL4O7QeQdImkOkl1zc3NJQrfzGzwUUSU9g2SGcc9EXFiRt9rSE5PnQw0AMcDHwOOA+ZGxEfT/S4E3kQyu3goPU2FpEnAvVljF6qtrY26urpexX/ljZt7dVxXrr306KKPaWZWbJLqI6K2Y3tJT1V1JyK2AfMBJAl4GlgHVACTCnadCGwEtgCjJQ1LZx2t7WZm1k/KeqpK0uiCq6I+CixPi8kfgKnpFVQjgAuAuyOZHj0AnJceczFwV3/HbWY2mJV0xiFpETAHGCdpA3AVMBwgIm4CXg/cLCmA1aRXSEXEPkmfAH4JDAUWRMTqdNgrgNskfQH4I/D9UuZgZmbtlfqqqnnd9D8ITOuk7+fAzzPa1wGnFiVAMzPLrdxXVZmZ2QDjwmFmZrm4cJiZWS4uHGZmlosLh5mZ5eLCYWZmubhwmJlZLi4cZmaWiwuHmZnl4sJhZma5uHCYmVkuLhxmZpaLC4eZmeXiwmFmZrm4cJiZWS4uHGZmlosLh5mZ5eLCYWZmubhwmJlZLiUrHJIWSNosaVUn/VWSfibpEUmrJc1P298qaWXB6xVJ56Z9CyU9XdA3q1Txm5lZtmElHHshcAPww076LwPWRMQ7JB0FPCHp1oh4AJgFIGkM8CRwX8Fxn4mI20sXtpmZdaVkhSMilkua0tUuQKUkAaOArcC+DvucB9wbETv7EktzSwuLly7L7JsxbSqTq6sBWN/UREPj2nb9Y0fuadvesmNm23ZVRSPDhuzKHPOVvWPYsWcSAEOH7GR0RfsxFy8d0bY9u+YUqiorAWh4opH1mzZljllVOYrZNTUFY2Tn05OcCp095/S27RX19TS3bM/cb/KECcyYPg1Ivp8r6h/udEzn5JzAOR0qOWUp5xrHDcDrgSagAbg8IvZ32OcCYFGHti9KelTS1yUd1tngki6RVCeprrm5uaiBm5kNZoqI0g2ezDjuiYgTM/rOA04DPgW8DlgCzIyIbWn/BOBRoDoi9ha0PQ+MAL4DPBUR/95dHLW1tVFXV9erHK68cXOvjuvKtZceXfQxzcyKTVJ9RNR2bC/njGM+cEckngSeBo4v6H8/cGdr0QCIiE3p/ruBHwCn9mvEZmZW1sKxHjgTQNJ4YDqwrqB/Hh1OU6UzDtJ1kXOBzCu2zMysdEq2OC5pETAHGCdpA3AVMBwgIm4CrgEWSmoABFwRES+lx04BJgEdV3duTa/AErAS+MdSxW9mZtlKeVXVvG76m4CzOul7Bjgmo/2MogRnZma95k+Om5lZLi4cZmaWiwuHmZnl4sJhZma5uHCYmVkuLhxmZpaLC4eZmeXiwmFmZrm4cJiZWS4uHGZmlosLh5mZ5eLCYWZmubhwmJlZLi4cZmaWiwuHmZnl4sJhZma5uHCYmVkuLhxmZpaLC4eZmeXiwmFmZrmUtHBIWiBps6RVnfRXSfqZpEckrZY0v6DvVUkr09fdBe2vlfQ7SU9K+omkEaXMwczM2iv1jGMhMLeL/suANRExE5gDfLWgEOyKiFnp650Fx3wJ+HpEHAf8CfhI8cM2M7PODCvl4BGxXNKUrnYBKiUJGAVsBfZ1tnO63xnA36dNNwNXA9/uKo7mlhYWL12W2Tdj2lQmV1cDsL6piYbGte36x47c07a9ZcfMtu2qikaGDdmVOeYre8ewY88kAIYO2cnoivZjLl7650nS7JpTqKqsBKDhiUbWb9qUOWZV5Shm19QUjJGdT09yKnT2nNPbtlfU19Pcsj1zv8kTJjBj+jQg+X6uqH+40zGdk3MC53So5JSl3GscNwCvB5qABuDyiNif9h0uqU7SQ5LOTdvGAi9HRGtx2QAckzWwpEvS4+uam5tLmIKZ2eCiiCjtGyQzjnsi4sSMvvOA04BPAa8DlgAzI2KbpGMiYqOkvwB+BZwJNAMPpaepkDQJuDdr7EK1tbVRV1fXq/ivvHFzr47ryrWXHl30Mc3Mik1SfUTUdmwv94xjPnBHJJ4EngaOB4iIjem/64ClwMnAFmC0pNZTbBOBjf0dtJnZYFbuwrGeZCaBpPHAdGCdpCMlHZa2jyOZlayJZHr0AHBeevzFwF39HrWZ2SBW0sVxSYtIrpYaJ2kDcBUwHCAibgKuARZKagAEXBERL0l6C/CfkvaTFLfrImJNOuwVwG2SvgD8Efh+KXMwM7P2Sn1V1bxu+puAszLafwvM6OSYdcCpRQnQzMxyK/epKjMzG2BcOMzMLBcXDjMzy6VHhUPSSElD0u1pkt4paXhpQzMzs4NRT2ccy0k+yX0McB9wIcl9qMzMbJDpaeFQROwE3gPcGBHvA95QurDMzOxg1ePCIekvgQ8Ai9O2oaUJyczMDmY9LRyfBD4H3BkRq9P7Rz1QurDMzOxg1aMPAEbEMmAZQLpI/lJE/HMpAzMzs4NTT6+q+rGk10gaCawC1kj6TGlDMzOzg1FPT1WdEBHbgHOBe4HXklxZZWZmg0xPC8fw9HMb5wJ3R8Rekqf3mZnZINPTwvGfwDPASGC5pGOBbaUKyszMDl49XRz/JvDNgqZnJb21NCGZmdnBrKeL41WSvtb6DG9JXyWZfZiZ2SDT01NVC4AW4P3paxvwg1IFZWZmB6+ePsjpdRHx3oKv/03SylIEZGZmB7eezjh2SZrd+oWk04BdpQnJzMwOZj2dcfwj8ENJVenXfwIuLk1IZmZ2MOvRjCMiHomImcBJwEkRcTJwRlfHSFogabOkVZ30V0n6maRHJK2WND9tnyXpwbTtUUnnFxyzUNLTklamr1k9ztTMzIoi1xMAI2Jb+glygE91s/tCYG4X/ZcBa9KCNAf4qqQRwE7gooh4Q3r8NySNLjjuMxExK315ncXMrJ/19FRVFnXVGRHLJU3pahegUpKAUcBWYF9ENBaM0SRpM3AU8HIfYjUzsyLpS+Ho6y1HbgDuBpqASuD8iNhfuIOkU4ERwFMFzV+U9HngfuCzEbG7uzdqbmlh8dJlmX0zpk1lcnU1AOubmmhoXNuuf+zIPW3bW3bMbNuuqmhk2JDs6wNe2TuGHXsmATB0yE5GV7Qfc/HSEW3bs2tOoaqyEoCGJxpZv2lT5phVlaOYXVNTMEZ2Pj3JqdDZc05v215RX09zy/bM/SZPmMCM6dOA5Pu5ov7hTsd0Ts4JnNOhklOWLk9VSWqRtC3j1QJU53qnA/0dsDIdZxZwg6TXFLz3BOBHwPyCgvI54HjgjcAY4IouYr+k9QOLzc3NfQzVzMxaKaJ09ypMT1XdExEnZvQtBq6LiF+nX/+KZAbx+7SALAWujYjbOxl7DvCvEXFOd3HU1tZGXV1dr3K48sbNvTquK9deenTRxzQzKzZJ9RFR27E91+J4ka0HzgSQNB6YDqxLF8jvBH7YsWiksxDSdZFzSZ4NYmZm/agvaxxdkrSI5GqpcZI2AFcBwwEi4ibgGmChpAaShfYrIuIlSR8E/hoYK+lD6XAfSq+gulXSUen+K0k+X2JmZv2oZIUjIuZ1098EnJXRfgtwSyfHdPnZETMzK71ynqoyM7MByIXDzMxyceEwM7NcXDjMzCwXFw4zM8vFhcPMzHJx4TAzs1xcOMzMLBcXDjMzy8WFw8zMcnHhMDOzXFw4zMwsFxcOMzPLxYXDzMxyceEwM7NcXDjMzCwXFw4zM8vFhcPMzHIp2aNjLZ8rb9xc9DGvvfTooo9pZuYZh5mZ5VLSwiFpgaTNklZ10l8l6WeSHpG0WtL8gr6LJa1NXxcXtNdIapD0pKRvSlIpczAzs/ZKPeNYCMztov8yYE1EzATmAF+VNELSGOAq4E3AqcBVko5Mj/k28DFgavrqanwzMyuykq5xRMRySVO62gWoTGcNo4CtwD7g74AlEbEVQNISYK6kpcBrIuKhtP2HwLnAvV3F0dzSwuKlyzL7ZkybyuTqagDWNzXR0Li2Xf/YkXvatrfsmNm2XVXRyLAhuzLHfGXvGHbsmQTA0CE7GV3RfszFS0e0bc+uOYWqykoARo54jsOHb80cc9/+Cpp3TSuI65HM/QC2757I7n1jO82p0NlzTm/bXlFfT3PL9sz9Jk+YwIzpyfs3t7Swov7hTscszKnhiUbWb9qUuV9V5Shm19S0fd3ZfyPo/r9TIefknMA5FSunLOVeHL8BuBtoAiqB8yNiv6RjgOcK9tsAHJO+NmS0H0DSJcAlAEeNH1/8yM3MBilFRGnfIJlx3BMRJ2b0nQecBnwKeB2wBJhJ8gv/8Ij4Qrrf/wR2AUuB6yLib9L2vwKuiIhzuoqhtrY26urqehV/f13t5KuqzOxgI6k+Imo7tpf7qqr5wB2ReBJ4Gjge2AhMKthvYtq2Md3u2G5mZv2k3IVjPXAmgKTxwHRgHfBL4CxJR6aL4mcBv4yITcA2SW9O10UuAu4qT+hmZoNTSdc4JC0iuVpqnKQNJFdKDQeIiJuAa4CFkhoAkZx2eik99hrgD+lQ/966UA5cSnK1VgXJoniXC+NmZlZcpb6qal43/U0ks4msvgXAgoz2OuCA9RIzM+sf5T5VZWZmA4wLh5mZ5eLCYWZmubhwmJlZLi4cZmaWiwuHmZnl4sJhZma5uHCYmVkuLhxmZpaLC4eZmeXiwmFmZrm4cJiZWS4uHGZmlosLh5mZ5eLCYWZmubhwmJlZLiV9kJMdfK68cXNRx7v20qOLOp6ZHfw84zAzs1xcOMzMLBcXDjMzy6VkhUPSAkmbJa3qpP8zklamr1WSXpU0RtL0gvaVkrZJ+mR6zNWSNhb0vb1U8ZuZWbZSLo4vBG4AfpjVGRFfAb4CIOkdwL9ExFZgKzArbR8KbATuLDj06xHxH6UL28zMulKywhERyyVN6eHu84BFGe1nAk9FxLN9iaW5pYXFS5dl9s2YNpXJ1dUArG9qoqFxbbv+sSP3tG1v2TGzbbuqopFhQ3ZljvnK3jHs2DMJgKFDdjK6ov2Yi5eOaNueXXMKVZWVAIwc8RyHD9+aOea+/RU075pWENcjmfsBbN89kd37xmbmVJhPMXIqzCUrp4YnGlm/aVPmmFWVo5hdU9P2dWf/jaD7/06Fzp5zetv2ivp6mlu2Z+43ecIEZkxPvqfNLS2sqH+40zGdk3OCwZlTlrKvcUg6ApgL/DSj+wIOLCifkPRoeirsyC7GvURSnaS65ubmIkZsZja4KSJKN3gy47gnIk7sYp/zgQ9GxDs6tI8AmoA3RMQLadt44CUggGuACRHx4e7iqK2tjbq6ul7lUOzPPUD2Zx8G6vv4cxxmhy5J9RFR27G97DMOsmcVAG8DHm4tGgAR8UJEvBoR+4HvAqf2U4xmZpYqa+GQVAWcDtyV0X3AuoekCQVfvhvIvGLLzMxKp2SL45IWAXOAcZI2AFcBwwEi4qZ0t3cD90XEjg7HjgT+FviHDsN+WdIsklNVz2T0m5lZiZXyqqp5PdhnIcllux3bdwBjM9ovLEZsZmbWe77JoZWEF+HNDl0Hw+K4mZkNIC4cZmaWiwuHmZnl4sJhZma5uHCYmVkuvqrKBqz+uk2LmbXnGYeZmeXiwmFmZrm4cJiZWS5e4zDrhtdSzNrzjMPMzHJx4TAzs1x8qsrsIOFTYjZQeMZhZma5eMZhNsj4lvfWVy4cZlYSLlCHLhcOMxuwvC5UHi4cZmbd6K8CNVAKYckWxyUtkLRZ0qpO+j8jaWX6WiXpVUlj0r5nJDWkfXUFx4yRtETS2vTfI0sVv5mZZSvlVVULgbmddUbEVyJiVkTMAj4HLIuIrQW7vDXtry1o+yxwf0RMBe5PvzYzs35UslNVEbFc0pQe7j4PWNSD/d4FzEm3bwaWAld0d1BzSwuLly7L7JsxbSqTq6sBWN/UREPj2nb9Y0fuadvesmNm23ZVRSPDhuzKHPOVvWPYsWcSAEOH7GR0RfsxFy8d0bY9u+YUqiorARg54jkOH76VLPv2V9C8a1pBXI9k7gewffdEdu8bm5lTYT7FyKkwl8KcWhUrp/VNbzjgv1PHXIqRU1Y+Y0fu4eVdU3l1/xFFy2nx0hEH/OyNHbk6c9++5JT1c9/6fStWToXfs8KcDhu2hVGHbShqTs0tLayof/iAXFoVI6eOPwMzpk2l9VdlMXOCow/IKetnuq85Zf1Md/d7rztl/xyHpCNIZiY/LWgO4D5J9ZIuKWgfHxGb0u3ngfFdjHuJpDpJdc3NzUWP28xssDoYFsffAfymw2mq2RGxUdLRwBJJj0fE8sKDIiIkRWeDRsR3gO8A1NbWxtlzTu82kMnV1W1VuNVvOlmsKqzsXXl1/xHt/goBOHtO9mLVjj2T2v6y6k7HMTvTMafO8oHe5dRZLpD8pVWsnCZXH12wneTUVS6t8uaUlU/H9ylGTh3fZ3J1NVt29Ox/xzw5Zf3cZ33f+pJTZz8Du/eNbZv5dqenOVVVVrbLqaufgd7mlJ1P8j6lzqm7n+ne5NT5/6OJrN973Sn7jAO4gA6nqSJiY/rvZuBO4NS06wVJEwDSf4t/CYKZmXWprIVDUhVwOnBXQdtISZWt28BZQOuVWXcDF6fbFxceZ2Zm/aNkp6okLSJZyB4naQNwFTAcICJuSnd7N3BfROwoOHQ8cKek1vh+HBG/SPuuA/5L0keAZ4H3lyp+MzPLVsqrqub1YJ+FJJftFratAzJPDEfEFuDMIoRnZma9dDCscZiZ2QDiwmFmZrm4cJiZWS4uHGZmlosLh5mZ5eLCYWZmuSii07t2HDIkvUjyuY9DwTjgpXIHUUSHUj6HUi7gfA5m/ZXLsRFxVMfGQVE4DiWS6jrcan5AO5TyOZRyAedzMCt3Lj5VZWZmubhwmJlZLi4cA893yh1AkR1K+RxKuYDzOZiVNRevcZiZWS6ecZiZWS4uHGZmlosLxwAhaZKkByStkbRa0uXljqmvJA2V9EdJ95Q7lr6SNFrS7ZIel/SYpL8sd0x9Ielf0p+zVZIWSTq83DH1lKQFkjZLWlXQNkbSEklr03+PLGeMeXSSz1fSn7VHJd0paXR/xuTCMXDsAz4dEScAbwYuk3RCmWPqq8uBx8odRJFcD/wiIo4neZ7MgM1L0jHAPwO1EXEiMJTkEc8DxUJgboe2zwL3R8RU4P7064FiIQfmswQ4MSJOAhqBz/VnQC4cA0REbIqIh9PtFpJfTMeUN6rekzQROBv4Xrlj6av0Ech/DXwfICL2RMTL5Y2qz4YBFZKGAUcATWWOp8ciYjmwtUPzu4Cb0+2bgXP7Nag+yMonIu6LiH3plw8BE/szJheOAUjSFOBk4HfljaRPvgH8d2B/uQMpgtcCLwI/SE+9fU/SyHIH1VsRsRH4D2A9sAlojoj7yhtVn42PiE3p9vMkj6g+VHwYuLc/39CFY4CRNAr4KfDJiNhW7nh6Q9I5wOaIqC93LEUyDDgF+HZEnAzsYGCdCmknPf//LpKCWA2MlPTB8kZVPJF8BuGQ+ByCpP9Bchr71v58XxeOAUTScJKicWtE3FHuePrgNOCdkp4BbgPOkHRLeUPqkw3AhohonQHeTlJIBqq/AZ6OiBcjYi9wB/CWMsfUVy9ImgCQ/ru5zPH0maQPAecAH4h+/kCeC8cAIUkk59Afi4ivlTuevoiIz0XExIiYQrLo+quIGLB/0UbE88BzkqanTWcCa8oYUl+tB94s6Yj05+5MBvBif+pu4OJ0+2LgrjLG0meS5pKc6n1nROzs7/d34Rg4TgMuJPnrfGX6enu5g7I2/wTcKulRYBZwbZnj6bV05nQ78DDQQPJ7YsDcrkPSIuBBYLqkDZI+AlwH/K2ktSQzquvKGWMeneRzA1AJLEl/F9zUrzH5liNmZpaHZxxmZpaLC4eZmeXiwmFmZrm4cJiZWS4uHGZmlosLh1kRSHq14DLplZKK9slxSVMK74xqVm7Dyh2A2SFiV0TMKncQZv3BMw6zEpL0jKQvS2qQ9HtJx6XtUyT9Kn2ewv2SJqft49PnKzySvlpv9TFU0nfTZ2TcJ6mibEnZoOfCYVYcFR1OVZ1f0NccETNIPu37jbTtfwM3p89TuBX4Ztr+TWBZRMwkud/V6rR9KvCtiHgD8DLw3hLnY9Ypf3LcrAgkbY+IURntzwBnRMS69CaVz0fEWMNa/pUAAADSSURBVEkvARMiYm/avikixkl6EZgYEbsLxpgCLEkfQoSkK4DhEfGF0mdmdiDPOMxKLzrZzmN3wfareH3SysiFw6z0zi/498F0+7f8+XGsHwB+nW7fD3wc2p7JXtVfQZr1lP9qMSuOCkkrC77+RUS0XpJ7ZHrX3N3AvLTtn0ieGPgZkqcHzk/bLwe+k94B9VWSIrIJs4OI1zjMSihd46iNiJfKHYtZsfhUlZmZ5eIZh5mZ5eIZh5mZ5eLCYWZmubhwmJlZLi4cZmaWiwuHmZnl8v8B3d0zQQD3wXcAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot the loss with some scaling\n",
        "plt.bar([i for i in range(1,13)], \n",
        "        [(loss / len(ScFi.data)) for loss in model_SciFi.losses],\n",
        "        color='royalblue', \n",
        "        alpha=0.7)\n",
        "\n",
        "plt.grid(color='#95a5a6',\n",
        "         linestyle='--',\n",
        "         linewidth=2,\n",
        "         axis='y',\n",
        "         alpha=0.7)\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim(1.74, 1.93)\n",
        "plt.title('Scaled Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpbybXPveqDL"
      },
      "source": [
        "#Model Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApITbBFxwRpY"
      },
      "source": [
        "#Model Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKVW2Nb40jdt"
      },
      "source": [
        "# 1. Evaluation of the TripAdvisor embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6zUZm1I32qJ",
        "outputId": "e6130eb1-c6d0-43eb-f2f9-2a4d960e85c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('hotel', 49886), ('room', 35367), ('not', 31712), ('great', 21484), ('t', 19552), ('n', 19103), ('good', 17424), ('staff', 16641), ('stay', 15415), ('did', 14008), ('just', 12669), ('nice', 12647), ('rooms', 12412), ('no', 11853), ('location', 11359), ('stayed', 10502), ('service', 10377), ('night', 10170), ('time', 10134), ('beach', 10072), ('day', 9988), ('breakfast', 9738), ('clean', 9599), ('food', 9428), ('like', 8254), ('resort', 8154), ('place', 7792), ('really', 7790), ('the', 7615), ('pool', 7584), ('friendly', 6894), ('people', 6843), ('small', 6597), ('little', 6261), ('walk', 6256), ('got', 6206), ('excellent', 6193), ('area', 6121), ('best', 5750), ('helpful', 5708), ('bar', 5582), ('restaurant', 5341), ('restaurants', 5144), ('bathroom', 5107), ('water', 5039), ('trip', 5027), ('bed', 5002), ('recommend', 4865), ('view', 4737), ('beautiful', 4735), ('floor', 4696), ('went', 4680), ('comfortable', 4558), ('desk', 4476), ('nights', 4407), ('check', 4328), ('right', 4292), ('want', 4200), ('way', 4195), ('free', 4188), ('hotels', 4164), ('better', 4154), ('city', 4141), ('away', 4120), ('wonderful', 4108), ('make', 4080), ('ç', 3943), ('booked', 3895), ('price', 3881), ('bit', 3880), ('reviews', 3786), ('large', 3758), ('street', 3744), ('minutes', 3722), ('buffet', 3604), ('say', 3577), ('quite', 3557), ('new', 3553), ('days', 3495), ('lobby', 3470), ('experience', 3356), ('loved', 3276), ('going', 3255), ('morning', 3243), ('close', 3193), ('definitely', 3165), ('airport', 3063), ('big', 3054), ('shower', 3053), ('lovely', 3050), ('fantastic', 3034), ('problem', 3027), ('th', 3009), ('perfect', 2958), ('think', 2956), ('took', 2952), ('we', 2945), ('bad', 2918), ('walking', 2912), ('week', 2907)]\n",
            "[('swears', 1), ('muggings', 1), ('tallers', 1), ('victimized', 1), ('barcelonins', 1), ('esquadra', 1), ('mossos', 1), ('attests', 1), ('blackmailed', 1), ('indemnified', 1), ('thereafter', 1), ('evasive', 1), ('elusive', 1), ('aftermath', 1), ('unbolted', 1), ('invalid', 1), ('fingerprinted', 1), ('moneybelt', 1), ('pilling', 1), ('gobcn', 1), ('esteem', 1), ('uplift', 1), ('butthat', 1), ('aerate', 1), ('nauseously', 1), ('neutralizer', 1), ('bylaws', 1), ('cedar', 1), ('shilla', 1), ('tullys', 1), ('transistion', 1), ('uwajimaya', 1), ('yesler', 1), ('dreamless', 1), ('viaduct', 1), ('locacated', 1), ('stadia', 1), ('kens', 1), ('befriended', 1), ('singapre', 1), ('afforadable', 1), ('togeter', 1), ('lushly', 1), ('enjoydkay', 1), ('bellstand', 1), ('bgj', 1), ('promixity', 1), ('washstand', 1), ('inventive', 1), ('missteps', 1), ('enthusiatically', 1), ('geckoes', 1), ('enforcing', 1), ('howie', 1), ('deni', 1), ('reverberated', 1), ('overabudance', 1), ('gauged', 1), ('munir', 1), ('overflows', 1), ('tribunal', 1), ('fuencarral', 1), ('huddled', 1), ('localed', 1), ('jadge', 1), ('chiinatown', 1), ('wthat', 1), ('powerboard', 1), ('crates', 1), ('accesscons', 1), ('avarege', 1), ('wanderful', 1), ('wangfunjing', 1), ('avellino', 1), ('uffizii', 1), ('viewd', 1), ('tuskany', 1), ('rejoiced', 1), ('cereales', 1), ('lastcry', 1), ('uphoistery', 1), ('deskwith', 1), ('crisscrossing', 1), ('brerakfast', 1), ('efficeint', 1), ('yogart', 1), ('rosetta', 1), ('renai', 1), ('alle', 1), ('wiling', 1), ('marigolds', 1), ('begonias', 1), ('bougainvilla', 1), ('potted', 1), ('vecchiio', 1), ('linoel', 1), ('shrub', 1), ('groundspeople', 1), ('duo', 1)]\n",
            "[('disinterest', 6.107236385345459), ('hiked', 6.416950702667236), ('duckie', 6.582708835601807), ('americain', 6.652702808380127), ('trhe', 6.663197040557861)]\n",
            "[('intensely', 5.816161155700684), ('humidor', 6.312104225158691), ('anemic', 6.437470436096191), ('itself', 6.555418491363525), ('possibly', 6.586799621582031)]\n",
            "[('trunk', 6.375006198883057), ('carols', 6.556601524353027), ('reflexology', 6.5666093826293945), ('notewhile', 6.629999160766602), ('dealt', 6.758902072906494)]\n",
            "[('wildly', 6.5498857498168945), ('gimmicky', 6.744287490844727), ('tascheles', 6.769628047943115), ('garni', 6.814040184020996), ('valid', 6.8208794593811035)]\n",
            "[('maidseemed', 6.977465629577637), ('clotted', 7.4205522537231445), ('alto', 7.427980422973633), ('wayyyyy', 7.476780891418457), ('barricaded', 7.558277606964111)]\n",
            "[('inched', 6.327345371246338), ('noone', 6.346874237060547), ('broccolini', 6.4609808921813965), ('cranes', 6.489243984222412), ('rouges', 6.566827297210693)]\n",
            "[('vaugirard', 6.875255107879639), ('greenbrier', 7.0506415367126465), ('auditorium', 7.262965202331543), ('tougher', 7.316408157348633), ('chronic', 7.319569110870361)]\n",
            "[('researches', 7.0149312019348145), ('edith', 7.021975040435791), ('mediaval', 7.0562615394592285), ('whatsoever', 7.070920467376709), ('uncomplaining', 7.071120738983154)]\n",
            "[('marriottdom', 6.5096893310546875), ('farmer', 6.561581611633301), ('culprit', 6.5734148025512695), ('alsoseparate', 6.574130535125732), ('lobsters', 6.590260028839111)]\n"
          ]
        }
      ],
      "source": [
        "# Import statments\n",
        "import torch.nn as nn\n",
        "from collections import Counter\n",
        "from itertools import product\n",
        "\n",
        "# Load the trained model\n",
        "model_tripAdvisor = torch.load('tripadvisor_model.pt', map_location='cpu')\n",
        "\n",
        "# Choose 3 nouns, 3 verbs, and 3 adjectives (some should occur frequently while others should be rare.)\n",
        "df_tr = pd.DataFrame.from_dict(trAd.data_X)\n",
        "\n",
        "# Find the most common and least common words (manually inspect to split into adjectives, verbs and nouns.)\n",
        "most_common = Counter(\" \".join(df_tr['Review']).split()).most_common(100)\n",
        "least_common = Counter(\" \".join(df_tr['Review']).split()).most_common()[:-100:-1] \n",
        "\n",
        "print(most_common)\n",
        "print(least_common)\n",
        "\n",
        "# hotel (49886), room (35367), muggings (1)\n",
        "nouns = ['hotel', 'room', 'muggings']\n",
        "\n",
        "# stayed (10502), did (14008), victimized (1)\n",
        "verbs = ['stayed', 'did', 'victimized']\n",
        "\n",
        "# good (17424), great (21484), elusive (1)\n",
        "adjectives = ['good', 'great', 'elusive']\n",
        "\n",
        "words_tr = nouns + verbs + adjectives\n",
        "word_to_index_tr = TrAd.word_to_ix\n",
        "index_to_word_tr = dict([(value, key) for key, value in word_to_index_tr.items()])\n",
        "\n",
        "#For each of the 9 chosen words, retrieve the 5 closest words according to our model.\n",
        "def get_closest_word_hotel(word, topn=5):\n",
        "  word_distance = []\n",
        "  emb = model_tripAdvisor.embeddings\n",
        "  pdist = nn.PairwiseDistance()\n",
        "  i = word_to_index_tr[word]\n",
        "  lookup_tensor_i = torch.tensor([i], dtype=torch.long)\n",
        "  v_i = emb(lookup_tensor_i)\n",
        "  for j in range(len(TrAd.word_to_ix)):\n",
        "    if j != i:\n",
        "      lookup_tensor_j = torch.tensor([j], dtype=torch.long)\n",
        "      v_j = emb(lookup_tensor_j)\n",
        "      word_distance.append((index_to_word_tr[j], float(pdist(v_i, v_j))))\n",
        "  word_distance.sort(key=lambda x: x[1])\n",
        "  return word_distance[:topn]\n",
        "\n",
        "# Print the results\n",
        "overview_tr = {}\n",
        "for word in words_tr:\n",
        "  close_words = get_closest_word_hotel(word)\n",
        "  overview_tr[word] = close_words\n",
        "s_tr  = pd.Series(overview_tr,index=overview_tr.keys())\n",
        "for row in s_tr:\n",
        "  print(row)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsB1jEhoGIL5"
      },
      "source": [
        "# 2. Evaluation of the Sci-Fi model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYGdnKsrGB5C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ccf6097-37fd-45f5-8109-0898ebce3ee8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('the', 71950), ('and', 32065), ('of', 30805), ('to', 29662), ('a', 29636), ('he', 20246), ('i', 19301), ('it', 18227), ('in', 17059), ('was', 16193), ('you', 14728), ('that', 14493), ('his', 11322), ('s', 10028), ('had', 9309), ('t', 8973), ('for', 8780), ('on', 8432), ('but', 7971), ('with', 7911), ('as', 7568), ('they', 7033), ('at', 7011), ('be', 6587), ('is', 6484), ('we', 6356), ('said', 6151), ('not', 5784), ('have', 5575), ('him', 5300), ('there', 5218), ('from', 5041), ('all', 4989), ('this', 4857), ('were', 4749), ('she', 4726), ('one', 4666), ('out', 4629), ('what', 4486), ('if', 4369), ('her', 4298), ('no', 4100), ('up', 4065), ('by', 3963), ('me', 3815), ('would', 3789), ('an', 3787), ('them', 3578), ('could', 3462), ('been', 3433), ('my', 3377), ('or', 3358), ('so', 3305), ('are', 3194), ('can', 3147), ('into', 3084), ('then', 3058), ('like', 2999), ('about', 2948), ('do', 2872), ('your', 2805), ('when', 2779), ('now', 2743), ('back', 2687), ('time', 2674), ('man', 2473), ('their', 2462), ('more', 2419), ('know', 2352), ('just', 2214), ('only', 2204), ('down', 2202), ('which', 2173), ('don', 2152), ('will', 2089), ('over', 2006), ('ll', 1997), ('some', 1964), ('get', 1951), ('who', 1948), ('did', 1884), ('here', 1858), ('other', 1855), ('any', 1841), ('two', 1829), ('d', 1813), ('how', 1797), ('than', 1779), ('way', 1753), ('re', 1753), ('its', 1751), ('see', 1707), ('through', 1664), ('m', 1657), ('us', 1635), ('even', 1633), ('our', 1632), ('first', 1626), ('right', 1604), ('too', 1569)]\n",
            "[('unctional', 1), ('hardbacks', 1), ('zonecity', 1), ('recalculates', 1), ('wamingly', 1), ('industrialists', 1), ('thrustors', 1), ('foundlings', 1), ('hisereans', 1), ('arabella', 1), ('wulf', 1), ('sloweddown', 1), ('brigands', 1), ('soths', 1), ('brak', 1), ('kynaoce', 1), ('sliare', 1), ('egging', 1), ('psychotically', 1), ('pugilist', 1), ('teat', 1), ('cosmologist', 1), ('mv', 1), ('tac', 1), ('nim', 1), ('luisi', 1), ('uncap', 1), ('anestha', 1), ('subunit', 1), ('shortname', 1), ('yearsago', 1), ('eoaee', 1), ('dreadnaught', 1), ('megatons', 1), ('cultist', 1), ('unbending', 1), ('crosswise', 1), ('ginnangu', 1), ('inauguration', 1), ('quarto', 1), ('batty', 1), ('unplugged', 1), ('palmed', 1), ('actrian', 1), ('percepif', 1), ('quad', 1), ('firefly', 1), ('airduct', 1), ('ventilation', 1), ('marbled', 1), ('eccrine', 1), ('trickles', 1), ('thirstily', 1), ('loners', 1), ('patrolcar', 1), ('boysburned', 1), ('readerbegan', 1), ('preserves', 1), ('ville', 1), ('sera', 1), ('assizes', 1), ('althou', 1), ('moonward', 1), ('deformation', 1), ('congested', 1), ('pollute', 1), ('disciplines', 1), ('voluntemd', 1), ('wrhe', 1), ('pavan', 1), ('protts', 1), ('abased', 1), ('trquble', 1), ('throated', 1), ('pathis', 1), ('itundred', 1), ('naiads', 1), ('contaminants', 1), ('popularized', 1), ('sharpthinking', 1), ('nicholls', 1), ('toilets', 1), ('gir', 1), ('chadwick', 1), ('ouriously', 1), ('bhtz', 1), ('musta', 1), ('vasth', 1), ('disappointingly', 1), ('deadpan', 1), ('coldbloodedness', 1), ('tly', 1), ('gh', 1), ('mannschen', 1), ('scufflings', 1), ('ouled', 1), ('narwhal', 1), ('cottonpicking', 1), ('excellently', 1)]\n",
            "[('tolerance', 7.287609577178955), ('minded', 7.4139204025268555), ('thea', 7.445655822753906), ('health', 7.555187225341797), ('drills', 7.561492443084717)]\n",
            "[('pneumatic', 6.639772415161133), ('spitball', 6.863219738006592), ('refillable', 7.156240940093994), ('vetterlein', 7.422805309295654), ('halator', 7.543807506561279)]\n",
            "[('showroom', 6.346076965332031), ('statlen', 6.368318557739258), ('graystones', 6.37567138671875), ('courtly', 6.59827995300293), ('doral', 6.641772270202637)]\n",
            "[('expect', 6.608395576477051), ('tiody', 6.657797813415527), ('hunti', 6.680017948150635), ('upstairs', 6.771357536315918), ('lui', 6.83225154876709)]\n",
            "[('roimd', 6.844076633453369), ('shires', 6.949284553527832), ('hatches', 7.004269123077393), ('noises', 7.009160041809082), ('cornsilk', 7.06504487991333)]\n",
            "[('intn', 6.150983810424805), ('nnodels', 6.499569892883301), ('patternless', 6.512481689453125), ('transfusion', 6.575992107391357), ('plums', 6.5915632247924805)]\n",
            "[('braziperuan', 5.986299514770508), ('gainfully', 6.247177600860596), ('tumult', 6.275301456451416), ('ewer', 6.531621932983398), ('limulus', 6.541576385498047)]\n",
            "[('monuments', 7.010705947875977), ('connective', 7.02701997756958), ('sweethearf', 7.174931526184082), ('prickling', 7.211423873901367), ('denser', 7.253138542175293)]\n",
            "[('chronicle', 6.2034077644348145), ('whereas', 6.315042972564697), ('manhandle', 6.504621982574463), ('caesium', 6.558823585510254), ('carefully', 6.593855381011963)]\n"
          ]
        }
      ],
      "source": [
        "# Load the trained model\n",
        "model_sciFi = torch.load('scifi_model.pt', map_location='cpu')\n",
        "\n",
        "# Choose 3 nouns, 3 verbs, and 3 adjectives (some should occur frequently while others should be rare.)\n",
        "df_sc = pd.DataFrame.from_dict(scFi.data_X)\n",
        "\n",
        "# Find the most common and least common words (manually inspect to split into adjectives, verbs and nouns.)\n",
        "most_common = Counter(\" \".join(df_sc['Text']).split()).most_common(100)\n",
        "least_common = Counter(\" \".join(df_sc['Text']).split()).most_common()[:-100:-1] \n",
        "\n",
        "print(most_common)\n",
        "print(least_common)\n",
        "\n",
        "\n",
        "nouns_sc = ['time', 'jokers', 'coldbloodedness']\n",
        "verbs_sc = ['had', 'was', 'shuffle']\n",
        "adjectives_sc = ['first', 'marbled', 'up']\n",
        "\n",
        "words_sc = nouns_sc + verbs_sc + adjectives_sc\n",
        "word_to_index_sc = ScFi.word_to_ix\n",
        "index_to_word_sc = dict([(value, key) for key, value in word_to_index_sc.items()])\n",
        "\n",
        "#For each of the 9 chosen words, retrieve the 5 closest words according to our model.\n",
        "def get_closest_word_scifi(word, topn=5):\n",
        "  word_distance = []\n",
        "  emb = model_sciFi.embeddings\n",
        "  pdist = nn.PairwiseDistance()\n",
        "  i = word_to_index_sc[word]\n",
        "  lookup_tensor_i = torch.tensor([i], dtype=torch.long)\n",
        "  v_i = emb(lookup_tensor_i)\n",
        "  for j in range(len(ScFi.word_to_ix)):\n",
        "    if j != i:\n",
        "      lookup_tensor_j = torch.tensor([j], dtype=torch.long)\n",
        "      v_j = emb(lookup_tensor_j)\n",
        "      word_distance.append((index_to_word_sc[j], float(pdist(v_i, v_j))))\n",
        "  word_distance.sort(key=lambda x: x[1])\n",
        "  return word_distance[:topn]\n",
        "\n",
        "overview_sc = {}\n",
        "for word in words_sc:\n",
        "  close_words = get_closest_word_scifi(word)\n",
        "  overview_sc[word] = close_words\n",
        "s_sc  = pd.Series(overview_sc,index=overview_sc.keys())\n",
        "for row in s_sc:\n",
        "  print(row)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6ht-9hO2Z4F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed9e02eb-aa26-41da-e97d-425ece9183a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hotel model predictions:\n",
            "[('isn', 6.793273448944092), ('fullerton', 6.8118205070495605), ('dealt', 6.854813575744629), ('seaona', 6.896491527557373), ('repairing', 6.975038528442383)]\n",
            "[('vaugirard', 6.875255107879639), ('greenbrier', 7.0506415367126465), ('auditorium', 7.262965202331543), ('tougher', 7.316408157348633), ('chronic', 7.319569110870361)]\n",
            "\n",
            "\n",
            "scifi model predictions:\n",
            "[('centaurean', 7.020124435424805), ('approvingly', 7.022852897644043), ('stupor', 7.143141746520996), ('trading', 7.2732439041137695), ('translating', 7.300394535064697)]\n",
            "[('stella', 5.507462024688721), ('encyclopedias', 5.593226909637451), ('earnestly', 5.618689060211182), ('baen', 5.844242095947266), ('giller', 6.0032243728637695)]\n"
          ]
        }
      ],
      "source": [
        "# Choose 2 words and retrieve their 5 closest neighbours according to both embeddings\n",
        "words = ['the', 'good']\n",
        "\n",
        "# According to hotel review-based embeddings\n",
        "overview_hotel = {}\n",
        "for word in words:\n",
        "  close_words = get_closest_word_hotel(word, topn=5)\n",
        "  overview_hotel[word] = close_words\n",
        "\n",
        "res_ht  = pd.Series(overview_hotel,index=overview_hotel.keys())\n",
        "print('hotel model predictions:')\n",
        "for row in res_ht:\n",
        "  print(row)\n",
        "print('\\n')\n",
        "\n",
        "# According to sci-fi review-based embeddings\n",
        "overview_scifi = {}\n",
        "for word in words:\n",
        "  close_words = get_closest_word_scifi(word, topn=5)\n",
        "  overview_scifi[word] = close_words\n",
        "\n",
        "res_sc  = pd.Series(overview_scifi,index=overview_scifi.keys())\n",
        "print('scifi model predictions:')\n",
        "for row in res_sc:\n",
        "  print(row)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}